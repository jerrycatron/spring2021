{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('conda': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Midterm Exam\n",
    "---\n",
    "#### Jerry Catron\n",
    "#### CMSC320 Spring 2021\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preliminaries\n",
    "---\n",
    ">### Question 1\n",
    "\n",
    ">>#### (a)\n",
    ">>> If one were to use the mean to describe the central tendency of salaries at a corporation it could be misleading due to the exorbitant salary of the executives. A reader might construe that the average employee at that corporation is making much more than they actually are, due to the outlier executive salaries.\n",
    "\n",
    ">>#### (b) \n",
    ">>>*Sample:* $ X = \\{1,1,2,2,3,3,3,8,8,9,9,10,10\\}$\n",
    "\n",
    ">>>Suppose this sample represents the satisfaction (1-10 scale) of people trying a new flavor of ice cream.\n",
    "\n",
    ">>>#### i. \n",
    ">>>> $\\overline{x} = \\frac{1+1+2+2+3+3+3+8+8+9+9+10+10}{13} = \\frac{69}{13} \\approx 5.3$\n",
    "\n",
    ">>>>  A reported mean of 5.3 might mislead someone into thinking the there was no general preference towards/against this flavor, when in fact is was very divisive. \n",
    "\n",
    ">>>#### ii. \n",
    ">>>> $median = X_{\\frac{n+1}{2}} = X_{7} = 3$\n",
    "\n",
    ">>>> A reported median of 3 might mislead someone into thinking no one likes this flavor, and so it will not be sent into production, when in reality just under half of the sample group loved the flavor, so there would be a devoted customer base.\n",
    "\n",
    ">>>#### iii.\n",
    ">>>> $range = max(X) - min(X) = 10 - 1 = 9$\n",
    "\n",
    ">>>> A reported range of 9 might mislead someone into thinking that everyone in the dispersion of data is very high when in fact the data is grouped into two tight clusters.\n",
    "\n",
    ">>#### (c)\n",
    ">>> One situation to use NumPy's ndarray over a Pandas Dataframe is when constructing a neural network. The processes of forward-propagation and back-propagation require numerous matrix operations with homoegeneous data types. ndarray's are optimized for these computations, and would perform much faster than a Pandas Dataframe.\n",
    "\n",
    ">$\\;$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Python\n",
    "---\n",
    ">### Question 2\n",
    ">>#### (a)\n",
    ">>>`list2a = [(x+x, x*x) for x in range (1,101)]`\n",
    "\n",
    ">>#### (b)\n",
    ">>>`list2b = [(x+x, x*x) for x in range (1,101) if (x % 3 == 0 or x % 5 == 0)]`\n",
    "\n",
    ">>#### (c)\n",
    ">>>#### i.\n",
    ">>>>`list2ci = list(map(lambda x: (x+x, x*x), range(1,101)))`\n",
    "\n",
    ">>>#### ii.\n",
    ">>>>`list2cii = list(map(lambda x: (x+x, x*x), list(filter(lambda x: x % 3 == 0 or x % 5 == 0, range(1,101)))))`\n",
    "\n",
    "\n",
    ">$\\;$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Graphs I\n",
    "---\n",
    ">### Question 3\n",
    ">>#### (a)\n",
    ">>> $\\begin{matrix} & \\begin{matrix}A & B & C & D & E & F & G & H\\end{matrix} \\\\\n",
    "      \\begin{matrix}A \\\\ B \\\\ C \\\\ D \\\\ E \\\\ F \\\\ G \\\\ H\\end{matrix} &\n",
    "        \\begin{vmatrix}\n",
    "    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "    1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "    0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0\\\\\n",
    "    0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "    0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n",
    "        \\end{vmatrix}\n",
    "     \\end{matrix}$\n",
    "\n",
    ">>#### (b)\n",
    ">>> $\\fbox{A}\\rightarrow\\fbox{C}\\fbox{/} \\\\\n",
    "     \\fbox{B}\\rightarrow\\fbox{A}\\rightarrow\\fbox{C}\\fbox{/} \\\\\n",
    "     \\fbox{C}\\rightarrow\\fbox{A}\\fbox{/} \\\\\n",
    "     \\fbox{D}\\rightarrow\\fbox{C}\\fbox{/} \\\\\n",
    "     \\fbox{E}\\rightarrow\\fbox{A}\\rightarrow\\fbox{B}\\rightarrow\\fbox{C}\\rightarrow\\fbox{D}\\fbox{/} \\\\\n",
    "     \\fbox{F}\\rightarrow\\fbox{D}\\rightarrow\\fbox{G}\\fbox{/} \\\\\n",
    "     \\fbox{G}\\rightarrow\\fbox{D}\\rightarrow\\fbox{F}\\fbox{/} \\\\\n",
    "     \\fbox{H}\\rightarrow\\fbox{C}\\rightarrow\\fbox{E}\\fbox{/}$\n",
    "\n",
    ">>#### (c)\n",
    ">>> The Adjacency Matrix would be more suitable if we very rarely add nodes but are frequently adding and removing edges from the graph. This is because adding/removing an edge in an Adjacency Matrix is O(1). Adding an edge to an Adjacency List is also O(1), but removing an edge is O(|N|) where |N| is the number of nodes in the graph.\n",
    "\n",
    ">>#### (d)\n",
    ">>>#### i.\n",
    ">>>> C\n",
    "\n",
    ">>>#### ii.\n",
    ">>>> E\n",
    "\n",
    ">>>#### iii.\n",
    ">>>> H\n",
    "\n",
    ">$\\;$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Graphs II\n",
    "---\n",
    "\n",
    ">### Question 4\n",
    ">>#### (a)\n",
    ">>>#### i.\n",
    ">>>>The highest *in-degree* would represent the research artifact that is cited by other artifacts the most.\n",
    "\n",
    ">>>#### ii.\n",
    ">>>> The highest *betweenness centrality* would mean that a particular research artifact is a foundational artifact that acts as a \"bridge\" between many other artifacts. It is on the most amount of shortest paths between artifacts. So in order for new research to be published, this artifact is important to consider.\n",
    "\n",
    ">>#### (b)\n",
    ">>>#### i.\n",
    ">>>>To determine which accounts are bots I would use a form of *degree centrality*. Basically I would track the ratio of $\\frac{\\sum_{all \\;out\\; edges} in-degree}{\\sum_{all \\;in \\;edges} in-degree}$. So the higher this ratio is, the more disproportionate the popularity of an account's \"following\" vs \"follwers\" is. The idea here is that bot accounts follow accounts that have a lot of followers, so they can respond to their posts that will be seen by a lot of people. However it is unlikely that a bot account's followers are are real/influential accounts that would have many followers.\n",
    "\n",
    ">>>#### ii.\n",
    ">>>> To determine the age of an account I would use *betweenness centrality*. My assumption here (which may be wrong) is that normal people follow a decently wide array of people. So accounts that have been around longer follow/have been followed by other older accounts that have become important in their own spheres of influence. Thus accounts that have a high betweenness centrality have been around long enough to follow accounts whose influence grew as the social network grew, and now connect the shortest paths between those wide array of accounts.\n",
    "\n",
    ">$\\;$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Missing Data\n",
    "---\n",
    ">### Question 5\n",
    ">> Suppose students coming out of the STAMP building are asked about their gender (M/F), their class (fresh, soph, junior, senior) and their GPA (above/below 3.0). Their responses are recorded as data below.\n",
    "\n",
    ">>#### (a) MCAR\n",
    ">>> It is possible to assume the missing data from this survey to be missing completely at random if the surveyor accidentally slipped on a banana and the survey responses flew in the air. In this process some of them were trampled, leaving footprints where certain data aused to be. There would be no relationship between the probability of the missing data for certain variables and the value of other variables.\n",
    "\n",
    ">>#### (b) MNAR\n",
    ">>> It is possible to assume the missing data from this survey to be missing not at random if some of the respondents did not fit into either gender category. There is certainly a relationship between people who don't identify as Male or Female and the likelihood of them not answering as Male or Female.\n",
    "\n",
    ">>#### (c) MAR\n",
    ">>> It is possible to assume the missing data from this survey to be missing at random if we could use the observed data to predict the missing data. Say for example many Freshmen are missing GPA data. We could attribute this to the fact that most first-semester freshmen have never seen their college transcript, so they don't know what their GPA is. There is a relationship between the probability of missing GPA data and the Class variable.\n",
    "\n",
    ">$\\;$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Natural Language Processing\n",
    "---\n",
    ">### Question 6\n",
    ">>#### (a)\n",
    ">>>#### i.\n",
    ">>>> 1. \"you\"\n",
    "2. \"i\"\n",
    "3. \"As\"\n",
    "4. \"oh\"\n",
    "5. \"You\"\n",
    "6. \"I\"\n",
    "\n",
    ">>>#### ii.\n",
    ">>>> **Pros**: Ignore capitalization helps when analyzing text from internet comments/texts, as kids these days love the apathetic vibe all lowercase put off. For example, comment in comment 1 \"you\" isn't capitalization at the start of the sentence, but in comment 5 it is. They are the same term used in the same context, but one is capitalized and one isn't. If trying to find the inverse term frequency of \"you\" without ignore capitalization we would have inaccurate results.\n",
    "\n",
    ">>>> **Cons**: Ignore capitalization would be hurtful if analyzing language that had frequent use of acronyms. For example if we were to analyze a review on a book about the DOM (Document Object Model) writtten by an Italian guy named Dom, ignore capitalization would make those terms equivalent,and our analysis would be less accurate.\n",
    "\n",
    ">>#### (b)\n",
    ">>> The term with the lowest inverse document frequency (ignoring capitalization) is \"I\" with a idf of log(2). The are many terms that only appear in one document. Each of these terms has an idf of log(6), which is the highest possible real value. I will choose the term that comes first, and say that \"are\" has the highest idf at log(6).\n",
    "\n",
    ">>#### (c)\n",
    ">>> Because I am tie-breaking by first appearance, the highest idf does not change if we are able identify the suject of pronouns. Thus the term \"are\" with an idf of log(6) still has the highest idf. However the term with the lowest idf changes from \"I\" with an idf of log(2) to the term \"you/your\" with an idf of log(3/2)\n",
    "\n",
    ">$\\;$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Feedback\n",
    "---\n",
    ">### Question 7\n",
    ">> To be honest I really racked my brain for some critique of this course but I am coming up short. From a professional and informative class webpage, to the fact we have a Discord server, to the memes, starting class 5 minutes late, extra reading material provided, in-class examples of required project work, and everything else; this has been the most student-friendly course I've taken at UMD. The only complaint I have is that it is not in person. But to be honest this class is what I look forward to working on when I get a break from my other classes. So thank you Jose and TAs! You all deserve an award."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}